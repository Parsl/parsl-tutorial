{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Introduction: Why Parallel Scripting?**\n",
    "\n",
    "Parsl is a parallel scripting library that supports the specification and execution of dataflow-based scripts comprised of external applications and/or Python functions.\n",
    "\n",
    "Prasl runs applications and Python functions concurrently as soon as their inputs are available, reducing the need for complex parallel programming. Parsl expresses workflow in a portable fashion: the same script can run on multicore computers, clusters, clouds, grids, and supercomputers.\n",
    "\n",
    "In this tutorial, you will be able to first try a few Parsl examples (examples 1-3) on your local machine, to get a sense of the library. Then, in examples 4-6 you will run similar workflows on any resource you may have access to, such as clouds (Amazon Web Services), Cray HPC systems, clusters etc, and see how more complex workflows can be expressed with Parsl scripts.\n",
    "\n",
    "Examples 4-6 can also be run on a local multicore machine if desired.\n",
    "\n",
    "To run the tutorial, ensure that Python (3.5+) and parsl 0.3 is installed on the machine you would be using to run the tutorial on. The tutorial can be run from within this Jupyter notebook or as independent Python scripts.\n",
    "\n",
    "**To install Parsl:**\n",
    "\n",
    "Install Parsl with Pip: \n",
    "    $ pip3 install parsl\n",
    "    \n",
    "**Or to install from source:**\n",
    "\n",
    "Download Parsl:\n",
    "\n",
    "    $ git clone https://github.com/Parsl/parsl.git parsl\n",
    "Install:\n",
    "    $ cd parsl\n",
    "\n",
    "    $ python3 setup.py install\n",
    "    \n",
    "\n",
    "Setup the Parsl tutorial:\n",
    "\n",
    "    $ git clone https://github.com/Parsl/parsl-tutorial.git parsl_tutorial\n",
    "\n",
    "    $ cd parsl_tutorial\n",
    "\n",
    "    $ bash setup.sh\n",
    "    \n",
    "Doing this will add the sample applications `simulate` and `stats` (mock \"science\" applications) and some other functionalities to your local $PATH for you to run the tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial Section One\n",
    "\n",
    "This section will be a walk-through of the getting a simple \"mock\" science application running using Parsl on your local machine (localhost)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: Run a single application using  Parsl\n",
    "\n",
    "The first Parsl script runs `simulate.sh` to generate a single random number. It writes the number to standard out.\n",
    "\n",
    "<img src=\"p1/pattern.png\", align=left>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from parsl import App, DataFlowKernel, ThreadPoolExecutor\n",
    "from parsl.dataflow.futures import Future\n",
    "\n",
    "# Define our workers and dfk.\n",
    "# In this case, we are running locally and specifying a max of 4\n",
    "# concurrent threads\n",
    "workers = ThreadPoolExecutor(max_workers=4)\n",
    "dfk = DataFlowKernel(executors=[workers])\n",
    "\n",
    "@App('bash', dfk)\n",
    "def mysim(stdout=\"output/p1.out\", stderr=\"output/p1.err\"):\n",
    "    \"\"\"Set this example up as a bash app by returning the \n",
    "    command line app to be called, in this case simulate\"\"\"\n",
    "    return \"app/simulate\"\n",
    "\n",
    "\n",
    "mysim().result()\n",
    "\n",
    "with open('output/p1.out', 'r') as f:\n",
    "    print(f.read())\n",
    "\n",
    "dfk.cleanup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debugging\n",
    "\n",
    "The easiest way to debug a Parsl application is via the Python stream logger. The following example shows how to enable such debugging. Information about app failures can be found in by looking at the stderr file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from parsl import App, DataFlowKernel, ThreadPoolExecutor\n",
    "from parsl.dataflow.futures import Future\n",
    "from parsl import set_stream_logger\n",
    "\n",
    "set_stream_logger()\n",
    "\n",
    "# Define our workers and dfk.\n",
    "# In this case, we are running locally and specifying a max of 4\n",
    "# concurrent threads\n",
    "workers = ThreadPoolExecutor(max_workers=4)\n",
    "dfk = DataFlowKernel(executors=[workers])\n",
    "\n",
    "@App('bash', dfk)\n",
    "def mysim(stdout=\"output/p1.out\", stderr=\"output/p1.err\"):\n",
    "    \"\"\"Set this example up as a bash app by returning the \n",
    "    command line app to be called, in this case simulate\"\"\"\n",
    "    return \"app/simulate\"\n",
    "\n",
    "\n",
    "mysim().result()\n",
    "\n",
    "with open('output/p1.out', 'r') as f:\n",
    "    print(f.read())\n",
    "\n",
    "dfk.cleanup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2: Run a single Python function using Parsl\n",
    "\n",
    "The second example mirrors the first, however instead of using an external application instead it uses a Python function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from parsl import App, DataFlowKernel, ThreadPoolExecutor\n",
    "from parsl.dataflow.futures import Future\n",
    "\n",
    "workers = ThreadPoolExecutor(max_workers=4)\n",
    "dfk = DataFlowKernel(executors=[workers])\n",
    "\n",
    "@App('python', dfk)\n",
    "def mysim():\n",
    "    from random import randint\n",
    "    \"\"\"Set this example up as a bash app by returning the \n",
    "    command line app to be called, in this case simulate\"\"\"\n",
    "    return randint(1,100)\n",
    "\n",
    "\n",
    "print(mysim().result())\n",
    "dfk.cleanup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 3:  Running an ensemble of many apps in parallel with a loop\n",
    "\n",
    "This script uses a Python for loop to run many concurrent simulations. Note: rather than rely on stdout the simulation app redirects the output to a specified file.\n",
    "\n",
    "<img src=\"p2/pattern.png\", align=left>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from parsl import App, DataFlowKernel, ThreadPoolExecutor\n",
    "from parsl.dataflow.futures import Future\n",
    "\n",
    "workers = ThreadPoolExecutor(max_workers=4)\n",
    "dfk = DataFlowKernel(executors=[workers])\n",
    "\n",
    "# Simulate application that redirects output to the\n",
    "# specified file\n",
    "@App('bash', dfk)\n",
    "def mysim(outputs=[], stdout=\"output/p3.out\", stderr=\"output/p3.err\"):\n",
    "    return 'app/simulate > {0}'.format(outputs[0])\n",
    "\n",
    "\n",
    "results = []\n",
    "for i in range(5):\n",
    "    out_file = \"output/p3_sim_{0}\".format(i)\n",
    "    results.append(mysim(outputs=[out_file]))\n",
    "\n",
    "    \n",
    "print (\"Job Status: {}\".format([r.done() for r in results]))\n",
    "\n",
    "# wait for all apps to complete\n",
    "[r.result() for r in results]\n",
    "\n",
    "print (\"Job Status: {}\".format([r.done() for r in results]))\n",
    "\n",
    "outputs = [r.outputs[0] for r in results]\n",
    "\n",
    "for o in outputs:\n",
    "    with open(o.filename, 'r') as f:\n",
    "        print(f.read().strip())\n",
    "        \n",
    "dfk.cleanup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 4: Analyzing results of a parallel ensemble\n",
    "\n",
    "After all the parallel simulations in an ensemble run have completed, it is typically necessary to gather and analyze their results with some kind of post-processing analysis program or script. p3.py introduces such a postprocessing step. In this case, the files created by all of the parallel runs of simulation.sh will be averaged by the trivial \"analysis application\" `stats.sh`:\n",
    "\n",
    "\n",
    "<img src=\"p3/pattern.png\", align=left>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from parsl import App, DataFlowKernel, ThreadPoolExecutor\n",
    "from parsl.dataflow.futures import Future\n",
    "\n",
    "workers = ThreadPoolExecutor(max_workers=4)\n",
    "dfk = DataFlowKernel(executors=[workers])\n",
    "\n",
    "@App('bash', dfk)\n",
    "def mysim(outputs=[],\n",
    "          stdout=\"output/p4_sim.out\",\n",
    "          stderr=\"output/p4_sim.err\"):\n",
    "    return 'app/simulate > {0}'.format(outputs[0])\n",
    "    \n",
    "\n",
    "@App('bash', dfk)\n",
    "def stats(inputs=[],\n",
    "          outputs=[],\n",
    "          stderr='output/p4_stats.err',\n",
    "          stdout='output/p4_stats.out'):\n",
    "    \"\"\"call stats cli utility with all simulations ans inputs\"\"\"\n",
    "    return \"app/stats {0} > {1}\".format(\" \".join(inputs), outputs[0])\n",
    "\n",
    "\n",
    "# call the simulation app 5 times\n",
    "results = []\n",
    "for i in range(5):\n",
    "    out_file = \"output/p4_sim_{0}\".format(i)\n",
    "    results.append(mysim(outputs=[out_file]))\n",
    "\n",
    "# collect the output data futures\n",
    "sim_outputs = [r.outputs[0] for r in results]\n",
    "\n",
    "# run the stats app\n",
    "s = stats(inputs=sim_outputs, outputs=[\"output/p4_stats.txt\"])\n",
    "\n",
    "s.result()\n",
    "\n",
    "with open('output/p4_stats.txt', 'r') as f:\n",
    "    print(f.read())\n",
    "\n",
    "dfk.cleanup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial Section Two\n",
    "\n",
    "This section introduces the aspects of running on remote computational resources. We will go into the configuration aspects that allow parsl to run your applications on computation resources. \n",
    "\n",
    "Parsl supports a variety of resource providers as well as methods for submitting workload to those resources. Parsl supports execution from login nodes (or nodes with access to submission queues) as well as remote hosts via SSH. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 5: Running a simple app using pilot jobs\n",
    "\n",
    "This script first generates a file with random numbers and then uses the `sort` application to sort those numbers. \n",
    "\n",
    "First we run the sort script using the local ThreadPoolExecutor. We will subsequently extend this example to submit the job via the IPyParallel pilot job model and then to a remote resource. \n",
    "\n",
    "### Example 5.a: Running the sort app locally using threads\n",
    "\n",
    "First we use the same approach as the prior examples to run the `sort` command with local threads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "from parsl import App, DataFlowKernel, ThreadPoolExecutor, IPyParallelExecutor\n",
    "from parsl.dataflow.futures import Future\n",
    "\n",
    "workers = ThreadPoolExecutor(max_workers=4)\n",
    "dfk = DataFlowKernel(executors=[workers])\n",
    "\n",
    "@App('bash', dfk)\n",
    "def sort(unsorted, \n",
    "         outputs=[],\n",
    "         stderr='output/p5_a_sort.err',\n",
    "         stdout='output/p5_a_sort.out'):\n",
    "    \"\"\"Call sort executable on file `unsorted`\"\"\"\n",
    "    return \"sort -g {} > {}\".format(unsorted, outputs[0])\n",
    "\n",
    "s = sort(\"input/unsorted.txt\", outputs=[\"output/a_sorted.txt\"])\n",
    "\n",
    "output_file = s.outputs[0].result()\n",
    "\n",
    "print(\"Contents of the unsorted.txt file:\")\n",
    "with open('input/unsorted.txt', 'r') as f:\n",
    "    print(f.read().replace(\"\\n\",\",\"))\n",
    "    \n",
    "print(\"\\nContents of the sorted output file:\")\n",
    "with open(output_file, 'r') as f:\n",
    "    print(f.read().replace(\"\\n\",\",\"))\n",
    "    \n",
    "dfk.cleanup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 5.b: Running the sort app locally using pilot jobs\n",
    "\n",
    "We now use IPyParallel to run the `sort` command using a pilot job model. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from parsl import App, DataFlowKernel, ThreadPoolExecutor, IPyParallelExecutor\n",
    "from parsl.dataflow.futures import Future\n",
    "import json\n",
    "from config.local import config\n",
    "\n",
    "dfk = DataFlowKernel(config=config)\n",
    "\n",
    "@App('bash', dfk)\n",
    "def sort(unsorted: str, \n",
    "         outputs: list = [],\n",
    "         stderr: str='output/p5_b_sort.err',\n",
    "         stdout: str='output/p5_b_sort.out')-> Future:\n",
    "    \"\"\"Call sort executable on file `unsorted`\"\"\"\n",
    "    return \"sort -g {} > {}\".format(unsorted, outputs[0])\n",
    "\n",
    "s = sort(\"input/unsorted.txt\", outputs=[\"output/b_sorted.txt\"])\n",
    "\n",
    "output_file = s.outputs[0].result()\n",
    "\n",
    "print(\"Contents of the unsorted.txt file:\")\n",
    "with open('input/unsorted.txt', 'r') as f:\n",
    "    print(f.read().replace(\"\\n\",\",\"))\n",
    "    \n",
    "print(\"\\nContents of the sorted output file:\")\n",
    "with open(output_file, 'r') as f:\n",
    "    print(f.read().replace(\"\\n\",\",\"))\n",
    "dfk.cleanup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 5.c: Running the sort app on a cluster using pilot jobs\n",
    "\n",
    "We now take the previous example and run it on a cluster. This example requires that you execute it on the login node with the entire tutorial repository. It also requires that the Parsl tutorial directory is available on the worker node via a shared file system.\n",
    "\n",
    "You will need the Python module and Parsl library. Instructions for setting up the environment are available online:  \n",
    "\n",
    "For Midway you can use our shared library:\n",
    "\n",
    "$ module load python/3.5.2+gcc-4.8; \n",
    "\n",
    "$ source /scratch/midway/yadunand/parsl_env_3.5.2_gcc/bin/activate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from parsl import App, DataFlowKernel, ThreadPoolExecutor, IPyParallelExecutor\n",
    "from parsl.dataflow.futures import Future\n",
    "import json\n",
    "import os\n",
    "\n",
    "from config.midway import config\n",
    "\n",
    "dfk = DataFlowKernel(config=config)\n",
    "\n",
    "@App('bash', dfk)\n",
    "def sort(unsorted, outputs=[]):\n",
    "    \"\"\"Call sort executable on file `unsorted`\"\"\"\n",
    "    return \"sort -g {} > {}\".format(unsorted, outputs[0])\n",
    "\n",
    "s = sort(os.path.abspath(\"input/unsorted.txt\"), \n",
    "         outputs=[os.path.abspath(\"output/sorted_c.txt\")])\n",
    "\n",
    "output_file = s.outputs[0].result()\n",
    "\n",
    "print(\"Contents of the unsorted.txt file:\")\n",
    "with open('input/unsorted.txt', 'r') as f:\n",
    "    print(f.read().replace(\"\\n\",\",\"))\n",
    "    \n",
    "print(\"\\nContents of the sorted output file:\")\n",
    "with open(output_file, 'r') as f:\n",
    "    print(f.read().replace(\"\\n\",\",\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 5.d: Running the sort app on remote resources using pilot jobs\n",
    "\n",
    "In the previous example we needed to be on a login node to run the application. Parsl also can be run over a remote SSH tunnel. However, it requires that the machine on which this application is running to also run the IPythonParallel controller and be accessible from the remote machine.  You will also need to configure your SSH agent to enable creation of the SSH connection to the login node.\n",
    "\n",
    "Note: Parsl does not yet support file staging. Instead, for this eaxmple we use a Python app to first create the unsorted file and then we use the Parsl SSH connection to stage the file back again.\n",
    "\n",
    "To run this script you must update the shared_dir variable. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from parsl import App, DataFlowKernel, ThreadPoolExecutor, IPyParallelExecutor\n",
    "from parsl.dataflow.futures import Future\n",
    "from parsl import set_stream_logger\n",
    "import json\n",
    "import os\n",
    "\n",
    "#set_stream_logger()\n",
    "from config.midway import config\n",
    "\n",
    "shared_dir = \"/scratch/midway/yadunand/tutorial\" # directory shared with worker nodes\n",
    "\n",
    "# Set remote connection details\n",
    "config[\"sites\"][0][\"auth\"]= {\"channel\": \"ssh\",\n",
    "                             \"hostname\":\"swift.rcc.uchicago.edu\",\n",
    "                             \"username\": \"yadunand\",\n",
    "                             \"scriptDir\": shared_dir}\n",
    "#print (config)\n",
    "dfk = DataFlowKernel(config=config)\n",
    "\n",
    "@App('python', dfk)\n",
    "def create_unsorted_file(outputs=[]):\n",
    "    from random import randint\n",
    "    file = open(outputs[0], 'w') \n",
    "    for i in range(0,50):\n",
    "        file.write(\"{0}\\n\".format(randint(1,100)))\n",
    "    file.close() \n",
    "\n",
    "@App('bash', dfk)\n",
    "def sort(unsorted, outputs=[]):\n",
    "    \"\"\"Call sort executable on file `unsorted`\"\"\"\n",
    "    return \"sort -g {0} > {1}\".format(unsorted, outputs[0])\n",
    "\n",
    "unsorted = create_unsorted_file(outputs=[os.path.join(shared_dir, \"unsorted-generated.txt\")])\n",
    "print (unsorted.outputs[0].result())\n",
    "\n",
    "s = sort(unsorted.outputs[0], \n",
    "         outputs=[os.path.join(shared_dir, \"sorted_d.txt\")])\n",
    "\n",
    "output_file = s.outputs[0].result()\n",
    "\n",
    "dfk.executor.execution_provider.channel.pull_file(output_file, '.')\n",
    "with open(os.path.basename(output_file), 'r') as f:\n",
    "     print(f.read().replace(\"\\n\",\",\"))\n",
    "        \n",
    "dfk.cleanup()      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 7: MPI Hello\n",
    "\n",
    "The final example is a basic \"Hello World!\" example that shows you how to run MPI applications. Here we have a simple MPI code mpi_hello.c that has each MPI rank sleep for a user-specified duration and then print the processor name on which the rank is executing followed by \"Hello World!\". \n",
    "\n",
    "The following script is designed to be run from the login node. If running locally you must first update the shared directory. Example configurations for other resources are included at the top of the script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "from parsl import App, IPyParallelExecutor, DataFlowKernel\n",
    "import os, json\n",
    "from parsl.dataflow.futures import Future\n",
    "\n",
    "# TODO : Remove after debugging\n",
    "from parsl import set_stream_logger\n",
    "set_stream_logger()\n",
    "\n",
    "\"\"\"From now on, the tutorial applications are written to run on Midway,\n",
    "a cluster located at the University of Chicago Research Computing Center\n",
    "They have also been tested locally on both Mac and Ubuntu Linux.\n",
    "In order to run them locally,\n",
    "either start an IPyParallel cluster controller on your machine\n",
    "or change the workers to something like this:\n",
    "workers = ThreadPoolExecutor(max_workers=NUMBER OF CORES)\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "from config.midway import config\n",
    "# Set remote connection details\n",
    "shared_dir = \"/scratch/midway/yadunand/tutorial\" # directory shared with worker nodes\n",
    "config[\"sites\"][0][\"auth\"]= {\"channel\": \"ssh\",\n",
    "                             \"hostname\":\"swift.rcc.uchicago.edu\",\n",
    "                             \"username\": \"yadunand\",\n",
    "                             \"scriptDir\": shared_dir}\n",
    "config[\"sites\"][0][\"execution\"][\"block\"][\"walltime\"] = '00:10:00'\n",
    "config[\"sites\"][0][\"execution\"][\"block\"][\"nodes\"] = 2\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "from config.cori import config\n",
    "shared_dir = \"/global/homes/y/yadunand/tutorial\" # directory shared with worker nodes\n",
    "config[\"sites\"][0][\"auth\"]= {\"channel\": \"ssh\",\n",
    "                             \"hostname\":\"cori.nersc.gov\",\n",
    "                             \"username\": \"yadunand\",\n",
    "                             \"scriptDir\": shared_dir}\n",
    "config[\"sites\"][0][\"execution\"][\"block\"][\"walltime\"] = '00:10:00'\n",
    "config[\"sites\"][0][\"execution\"][\"block\"][\"nodes\"] = 2\n",
    "\"\"\"\n",
    "\n",
    "from config.midway import config\n",
    "shared_dir = \"/scratch/midway/chard/parsl-tutorial\" # Update with your local path\n",
    "\n",
    "dfk = DataFlowKernel(config=config)\n",
    "\n",
    "@App('bash', dfk)\n",
    "def compile_app(dirpath, stdout=None, stderr=None, compiler=\"mpicc\"):\n",
    "    \"\"\"Compile mpi app with mpicc\n",
    "    On midway use compiler = mpicc\n",
    "    On Cori use default compiler= cc\n",
    "    \"\"\"\n",
    "    return '''cd {0}; make clean; make CC={1} '''.format(dirpath, compiler)\n",
    "\n",
    "@App('bash', dfk)\n",
    "def mpi_hello(dirpath, launcher=\"mpirun\", app=\"mpi_hello\", nproc=20, outputs=[]):\n",
    "    \"\"\"Call compiled mpi executable with mpilib.\n",
    "    Works natively for openmpi mpiexec, mpirun, orterun, oshrun, shmerun\n",
    "    mpiexec is default\"\"\"\n",
    "    if launcher == \"mpirun\" :        \n",
    "        return \"cd {}; {} -np {} {} &> {};\".format(dirpath, launcher, nproc, app, outputs[0] )\n",
    "    elif launcher == \"srun\" :\n",
    "        return \"cd {}; {} -n {} ./{} &> {};\".format(dirpath, launcher, nproc, app, outputs[0] )\n",
    "\n",
    "# use .result() to make the execution wait until the app has compiled\n",
    "compile_app(dirpath=os.path.join(shared_dir, \"mpi_apps\"),\n",
    "            stdout=os.path.join(shared_dir, \"mpi_apps.compile.out\"),\n",
    "            stderr=os.path.join(shared_dir, \"mpi_apps.compile.err\",),\n",
    "            compiler='mpicc'\n",
    "           ).result()\n",
    "\n",
    "\n",
    "hello = mpi_hello(os.path.join(shared_dir, \"mpi_apps\"),\n",
    "                  launcher=\"mpirun\",\n",
    "                  outputs=[os.path.join(shared_dir, \"mpi_apps\", \"hello.txt\")])\n",
    "\n",
    "output_file = hello.outputs[0].result()\n",
    "\n",
    "dfk.executor.execution_provider.channel.pull_file(output_file, '.')\n",
    "with open(os.path.basename(output_file), 'r') as f:\n",
    "     print(f.read())\n",
    "        \n",
    "os.remove(os.path.basename(output_file))\n",
    "dfk.cleanup()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
